Paper implementations for VAE and Attention, complete with reports. List of remaining quirks/tests to run

Attention:
Special case of google translate. Original attention code (not as good) included for historical documentation purposes.

VAE:
Correct implentations, manifolds correct. Code a bit outdated. Email me for fix--have a flight to catch and won't have access to my main machine over break. The bug in the old version was simply swatching relu to softplus.

Google Translate (multilingual translation): Correct multilingual implementation. Rigerous testing of correctness. Unable to reproduce results of original
